{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AST 7939 Homework Assignment #2 (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All work is due Wednesday February 22 at 5 pm.\n",
    "\n",
    "## Instruction: \n",
    "Do all homework in this Jupyter notebook and submit your final .ipynb file via Canvas. Show ALL your work and try to add comment lines as needed to describe what your code does. \n",
    "\n",
    "You are encouraged to discuss homework problems with your classmates. However, your python script and answers to the questions must be written by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Computational complexity (2 pts)\n",
    "\n",
    "We learned that different machine learning algorithms have different computational costs. Let's check this out using the two moons dataset (see the cell below for an example). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) While varying the number of data points in the training/test dataset from 100, 1000, 10000, to 100000, measure the time it takes for kNN, DT, and SVM to create a model (i.e., fit) and make predictions. For simplicity, instead of optimizing hyperparameters, use n_neighbors=10 for kNN, max_depth=4 for DT, and kernel=\"rbf\", C=1, gamma=1 for SVM. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# This is the size of the data.\n",
    "n = 100\n",
    "\n",
    "# This creates a training dataset.\n",
    "X_train, y_train = datasets.make_moons(n_samples=n, noise=0.3, random_state=0)\n",
    "\n",
    "# This creates a test dataset for prediction. \n",
    "X_pred, y_pred = datasets.make_moons(n_samples=n, noise=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time how long it takes using python\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Make a plot showing the running time vs. size of dataset for the three algorightms and for training/prediction -- you will have six curves in the end. Make sure your figure is readable. Discuss what you find from the plot. Using a logarithmic scale for both x and y axes will be helpful to interpret the result. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will have 6 times measured, make a plot of time vs number of datasets. For the three models kNN, DT and SVM. \n",
    "# Why is what you see happening in the diagram? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machine classifier (8 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build Support Vector Machines to determine the hyperplane separating Super-Earths and Sub-Neptunes, the so-called radius valley in planet radius vs. orbital period plots (see Fulton et al. 2017; https://ui.adsabs.harvard.edu/abs/2017AJ....154..109F/abstract). To determine accurate planet parameters from transit surveys, accurate stellar parameters are required, because the transit depth only constrains $R_p/R_*$, where $R_p$ and $R_*$ are the planetary and stellar radius. In Van Eylen et al. (2018; https://ui.adsabs.harvard.edu/abs/2018MNRAS.479.4786V/abstract), authors used a sample of exoplanet host stars with parameters homogeneously measured from asteroseismology, which can provide highly precise masses and radii for a sample of bright stars, and determined that the radius valley can be describe by ${\\rm log}_{10} R_p = m{\\rm log}_{10} P + a$, where $m=-0.09^{+0.02}_{-0.04}$ and $a=0.37^{+0.04}_{-0.02}$.\n",
    "\n",
    "In this homework problem, we will reproduce the SVM models of Van Eylen et al. (2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) First things first: let's collect the data presented in Table 1 of Van Eylen et al. (2018). One way to do this is to download the source file from the arXiv and read in the table (and do some cleaning). This can be tedious, but remember that making a ML model will always start with data acquisition and cleaning! When you submit your homework, upload the data file you compile so that I can reproduce your results. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No planets are in the radius gap, why? Atmospheres being stripped or rocky planets? \n",
    "# observational effect of planet radius vs orbital period \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Let's make a plot showing planet radius vs. orbital period, similar to Figure 2 of Van Eylen et al. (2018) but without additional data from Fulton et al. (2017). Similar to Figure 2 of Van Eylen et al. (2018), make two panels, one showing the entire data points and the other showing 1-100 days & 1-4 Earth radii to more clearly identify the radius valley. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate Figure 2, just need to plot the red dots, not the other data\n",
    "# Then have a zoom-in figure of the radius valley\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) We would like to fit the radius valley using the SVM. However, in order to use the SVM, we need a labelled data in the first place. Luckily, we have a well separated dataset. In Van Eylen et al. (2018), they took a few different approaches to fit the radius valley. Here, let's use one of the fits from bootstrapping, that is, if a planet's radius and orbital periods satisfy ${\\rm log}_{10} R_p > m{\\rm log}_{10} P + a$, where $m=-0.10$ and $a=0.38$, then we will label them as sub-Neptunes. If ${\\rm log}_{10} R_p \\leq m{\\rm log}_{10} P + a$, where $m=-0.10$ and $a=0.38$, we will label them as super-Earth. Using this criterion, make an array that contains the label (e.g., 0 if sub-Neptune, 1 if super-Earth).\n",
    "\n",
    "Then, repeat #2b and make similar plots, but this time make the data points color-coded by their labels. Also, add a line showing ${\\rm log}_{10} R_p = m{\\rm log}_{10} P + a$, where $m=-0.10$ and $a=0.38$. Your plot should look similar to Figure 5 of Van Eylen et al. (2018), except that you will have a single line dividing super-Earths and sub-Neptunes instead of multiple lines representing 20 different fits. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything above line is sub-neptune, everything below is super earth\n",
    "# make that classification an array\n",
    "# color code the points by their classification and overplot the line, recreate Fig 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Now that we have a labelled dataset, we are ready to build SVM models. Using orbital period and planet radius (both in log scale) as two features, make a few linear SVM models varying the panelty parameter C from 0.01, 0.1, 1, 10, 100, to 1000. Then, make six figures showing the data, hyperplane, and support vectors. Don't forget that SVMs need feature scaling. (2 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make SVM models similar to the ones in class with the hyperplane and support vectors \n",
    "# 6 models in total \n",
    "# draw hyperplane and mark support vectors\n",
    "# remember feature scaling \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Which penalty parameter $C$ do you think best seperates super-Earths and sub-Neptunes? And why? Write down an equation for the hyperplane in the form of ${\\rm log}_{10} R_p = m{\\rm log}_{10} P + a$. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which C fits best? Read the paper to see context\n",
    "# find the slope and intersect using SVM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Uncertainties on the slope $m$ and offset $a$ can be determined using bootstrapped samples ([random sampling with replacement](https://en.wikipedia.org/wiki/Bootstrapping_(statistics) )). Following Van Eylen et al. (2018), generate 1000 bootstrapped samples each of which has the same size as the original dataset. For each bootstrapped, compute the slope $m$ and offset $a$ of the hyperplane using the best $C$ you determined from #2e. Compute the mean and standard deviation of $m$ and $a$. What is your final hyperplane, and how does that compare with the hyperplane Van Eylen et al. (2018) determined? (2 pt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bootstrap to find uncertainty\n",
    "# will generate 1000 hyperplanes \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! With this homework, you essentially reproduced results presented in a journal paper. If you enjoyed the homework and would like to do some additional related analysis, I recommend you have a look at the following papers. Reproducing (and extending) the following papers would be a nice term project.\n",
    "\n",
    "https://ui.adsabs.harvard.edu/abs/2021AJ....161..265D/abstract\n",
    "\n",
    "https://ui.adsabs.harvard.edu/abs/2023MNRAS.519.4056H/abstract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
